{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-12T09:21:38.109132200Z",
          "start_time": "2024-02-12T09:21:37.034636600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yv6XpNydpoK0",
        "outputId": "355a960b-b21a-4ed2-be28-341d9480b303"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.optimizers.legacy import Adam\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "H4aTCcOHpoK2"
      },
      "source": [
        "# Objective\n",
        "This study endeavors to construct a predictive model for estimating the salaries of Data Science positions worldwide, denominated in US dollars (USD).\n",
        "In this project, a comprehensive dataset contained within a CSV file is explored, encompassing a rich array of information on job roles from a global perspective. The dataset comprises 3,756 records, each detailing various attributes associated with employment positions. These attributes serve as indicators for determining the annual salary (in USD) for each job listed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xTkResm9poK3"
      },
      "source": [
        "## Read Data From Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qea0iAazpoK3",
        "outputId": "af36ee53-634c-4bf8-a468-e1d2bb45260e"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./SalarayDataSet/ds_salaries.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Features\n",
        "The dataset contains the following features:\n",
        "\n",
        "1. **work_year**: The year the salary was paid.\n",
        "\n",
        "2. **experience_level**: The experience level in the job during the year.\n",
        "\n",
        "3. **employment_type**: The type of employment for the role.\n",
        "\n",
        "4. **job_title**: The role worked in during the year.\n",
        "\n",
        "5. **salary**: The total gross salary amount paid.\n",
        "\n",
        "6. **salary_currency**: The currency of the salary paid as an ISO 4217 currency code.\n",
        "\n",
        "7. **salary_in_usd**: The salary in USD.\n",
        "\n",
        "8. **employee_residence**: Employee's primary country of residence in during the work year as an ISO 3166 country code.\n",
        "\n",
        "9. **remote_ratio**: The overall amount of work done remotely.\n",
        "\n",
        "10. **company_location**: The country of the employer's main office or contracting branch.\n",
        "\n",
        "11. **company_size**: The median number of people that worked for the company during the year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA) - Salary Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Overview\n",
        "The dataset contains 3755 entries and 11 columns. Below is the description of each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification Of Non-Empty\n",
        "In order to encode and process the data so that it will be ready for the model, we verify there are no Nan cells.\n",
        "As shown bellow, all fileds are filled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[data.isna().any(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution of Salary (in USD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(data=data, kde=True, bins=30, x='salary_in_usd')\n",
        "plt.title('Distribution of Salary (in USD)')\n",
        "plt.xlabel('Salary (in USD)')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Salary by Position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "CN6TelxKpoK5",
        "outputId": "4f261306-81bd-4b52-a232-f2966fec9d54"
      },
      "outputs": [],
      "source": [
        "mean_salary_by_position = data.groupby('job_title')['salary_in_usd'].mean()\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "mean_salary_by_position.sort_values(ascending=False).plot(kind='bar')\n",
        "plt.title('Mean Salary by Position')\n",
        "plt.ylabel('Mean Salary (in USD)')\n",
        "plt.xlabel('Position')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution of Experience Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "jhRRtJvLBJaa",
        "outputId": "5e7ceb9b-b821-4c1d-bdb0-fbfbe1b5ea08"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.countplot(x='experience_level', data=data, order=data['experience_level'].value_counts().index)\n",
        "plt.title('Distribution of Experience Level')\n",
        "plt.xlabel('Experience Level')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution of Salary in USD by Work Year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.scatterplot(data=data, x='work_year', y='salary_in_usd', alpha=0.6, legend=False)\n",
        "plt.title('Salary vs Years of Experience')\n",
        "plt.xlabel('Years of Experience')\n",
        "plt.ylabel('Salary (in USD)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Normalizing and Preprocessing\n",
        "In order to normalize the data for the training, validation and testing of the model, we identify numeric and categorical features in the dataset and apply appropriate preprocessing transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "objects = data.select_dtypes(include='object')\n",
        "objects.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing Salary Columns\n",
        "This section normalizes the 'salary' and 'salary_in_usd' columns by dividing their values by 100,000. Normalization is performed to scale the values within a similar range, which can help improve the convergence of machine learning algorithms during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXbliFi_ZJcd"
      },
      "outputs": [],
      "source": [
        "cols_to_normalize = ['salary', 'salary_in_usd']\n",
        "data[cols_to_normalize] = data[cols_to_normalize] / 100000\n",
        "\n",
        "data[cols_to_normalize]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing and Splitting\n",
        "\n",
        "In this section, we preprocess the dataset and split it into training and validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature-Target Separation\n",
        "We separate the feature matrix `data_X` from the target variable `y` in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_X = data.drop('salary_in_usd', axis=1)\n",
        "y = data['salary_in_usd']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numeric and Categorical Feature Processing\n",
        "We identify numeric and categorical features in the dataset and apply appropriate preprocessing transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_features = data_X.select_dtypes(include=[np.number])\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "categorical_features = data_X.select_dtypes(include=['object'])\n",
        "categorical_transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features.columns),\n",
        "        ('cat', categorical_transformer, categorical_features.columns)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Splitting\n",
        "We split the dataset into training and validation sets using a specified test size and random state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indices = data.index\n",
        "train_indices, valid_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
        "x_train, y_train = data_X.loc[train_indices, :], y.loc[train_indices]\n",
        "x_valid, y_valid = data_X.loc[valid_indices, :], y.loc[valid_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Applying Transformers \n",
        "We apply the preprocessing transformers to the training and validation feature sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7Z-LMme-QqM",
        "outputId": "f8fa059e-2f88-4c3e-ba15-0f1f5a126fa1"
      },
      "outputs": [],
      "source": [
        "X_train_preprocessed = preprocessor.fit_transform(x_train)\n",
        "X_valid_preprocessed = preprocessor.transform(x_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XI_Pp4wOyc-"
      },
      "source": [
        "# Convolutional Neural Network (CNN) Modeling\n",
        "\n",
        "This section outlines the implementation of Convolutional Neural Network (CNN) model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Model Definition\n",
        "CNN model is defined using the Keras Sequential API. The model consists of convolutional layers with ReLU activation functions, max-pooling layers for downsampling, and dense layers for regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(248, 1)),\n",
        "    MaxPooling1D(2),\n",
        "\n",
        "    Conv1D(64, 3, activation='relu'),\n",
        "    MaxPooling1D(2),\n",
        "\n",
        "    Conv1D(128, 3, activation='relu'),\n",
        "    MaxPooling1D(2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Compilation\n",
        "The CNN model is compiled using the Adam optimizer, mean squared error (MSE) as the loss function, and mean absolute error (MAE) and mean squared error (MSE) as the metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mae', 'mse'])\n",
        "\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "The CNN model is trained on the preprocessed training data. The `fit` method is used to train the models with specified epochs, batch size, and verbose mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_model_history = cnn_model.fit(X_train_preprocessed, y_train,\n",
        "                                  validation_data=(X_valid_preprocessed, y_valid),\n",
        "                    epochs=400,\n",
        "                    batch_size=32,\n",
        "                    verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction \n",
        "After training, the performance of the model is evaluated on the validation set using MSE and MAE metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = cnn_model.predict(X_valid_preprocessed)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model's Prediction Performance\n",
        "When the data points closely align with the regression line, it indicates that the predicted values are close to the actual values, suggesting that the model's predictions are accurate. Conversely, if the data points are scattered far from the regression line, it suggests a larger discrepancy between the predicted and actual values, indicating poorer performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.regplot(x=y_valid, y=y_pred, line_kws={\"color\": \"red\"})\n",
        "plt.title('Actual vs Predicted Salary')\n",
        "plt.xlabel('Actual Salary')\n",
        "plt.ylabel('Predicted Salary')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "mse = mean_squared_error(y_valid, y_pred)\n",
        "axs[0].plot(cnn_model_history.history['mse'], label='Training MSE')\n",
        "axs[0].plot(cnn_model_history.history['val_mse'], label='Validation MSE')\n",
        "axs[0].set_title(f'Mean Squared Error (MSE) - {mse:.4f}')\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('MSE')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "axs[0].legend()\n",
        "\n",
        "mae = mean_absolute_error(y_valid, y_pred)\n",
        "axs[1].plot(cnn_model_history.history['mae'], label='Training MAE')\n",
        "axs[1].plot(cnn_model_history.history['val_mae'], label='Validation MAE')\n",
        "axs[1].set_title(f'Mean Absolute Error (MAE) - {mae:.4f}')\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Loss')\n",
        "axs[1].grid(True)\n",
        "axs[1].legend()\n",
        "\n",
        "plt.subplots_adjust(top=0.4, bottom=0.0)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
