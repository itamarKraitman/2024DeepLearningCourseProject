{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers.legacy import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./SalarayDataSet/ds_salaries.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "cols_to_normalize = ['salary', 'salary_in_usd']\n",
    "data[cols_to_normalize] = data[cols_to_normalize] / 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we preprocess the dataset and split it into training and validation sets. The following steps are performed:\n",
    "\n",
    "1. **Feature-Target Separation:** We separate the feature matrix `data_X` from the target variable `y` in the dataset.\n",
    "\n",
    "    ```python\n",
    "    data_X = data.drop('salary_in_usd', axis=1)\n",
    "    y = data['salary_in_usd']\n",
    "    ```\n",
    "\n",
    "2. **Numeric and Categorical Feature Processing:** We identify numeric and categorical features in the dataset and apply appropriate preprocessing transformations.\n",
    "\n",
    "    ```python\n",
    "    numeric_features = data_X.select_dtypes(include=[np.number])\n",
    "    numeric_transformer = StandardScaler()\n",
    "\n",
    "    categorical_features = data_X.select_dtypes(include=['object'])\n",
    "    categorical_transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features.columns),\n",
    "            ('cat', categorical_transformer, categorical_features.columns)\n",
    "        ])\n",
    "    ```\n",
    "\n",
    "3. **Dataset Splitting:** We split the dataset into training and validation sets using a specified test size and random state.\n",
    "\n",
    "    ```python\n",
    "    indices = data.index\n",
    "    train_indices, valid_indices = train_test_split(indices, test_size=0.4, random_state=42)\n",
    "    ```\n",
    "\n",
    "4. **Applying Transformers:** We apply the preprocessing transformers to the training and validation feature sets.\n",
    "\n",
    "    ```python\n",
    "    x_train, y_train = data_X.loc[train_indices, :], y.loc[train_indices]\n",
    "    x_valid, y_valid = data_X.loc[valid_indices, :], y.loc[valid_indices]\n",
    "\n",
    "    X_train_preprocessed = preprocessor.fit_transform(x_train)\n",
    "    X_valid_preprocessed = preprocessor.transform(x_valid)\n",
    "    ```\n",
    "\n",
    "These steps ensure that our dataset is properly preprocessed and ready for training and validation of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = data.drop('salary_in_usd', axis=1)\n",
    "y = data['salary_in_usd']\n",
    "\n",
    "numeric_features = data_X.select_dtypes(include=[np.number])\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = data_X.select_dtypes(include=['object'])\n",
    "categorical_transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features.columns),\n",
    "        ('cat', categorical_transformer, categorical_features.columns)\n",
    "    ])\n",
    "indices = data.index\n",
    "\n",
    "train_indices, valid_indices = train_test_split(indices, test_size=0.4, random_state=42)\n",
    "x_train, y_train = data_X.loc[train_indices, :], y.loc[train_indices]\n",
    "x_valid, y_valid = data_X.loc[valid_indices, :], y.loc[valid_indices]\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(x_train)\n",
    "X_valid_preprocessed = preprocessor.transform(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping Configuration\n",
    "We configure early stopping to monitor the validation loss and terminate training if the loss does not improve after a specified number of epochs.\n",
    "```python\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Modeling with and without Early Stopping\n",
    "\n",
    "In this section, we implement linear regression models with early stopping to prevent overfitting and improve generalization performance. The following steps are executed:\n",
    "\n",
    "1. **Building Linear Regression Model:** Two linear regression models are constructed using the Keras Sequential API. The first model (`linear_regression_early_stopping`) is equipped with early stopping, while the second model (`linear_regression`) is trained without early stopping.\n",
    "\n",
    "    ```python\n",
    "    linear_regression_early_stopping = Sequential([\n",
    "        Dense(1, input_shape=(X_train_preprocessed.shape[1],), activation='linear')\n",
    "    ])\n",
    "\n",
    "    linear_regression_early_stopping.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                                              loss='mean_squared_error',\n",
    "                                              metrics=['mae'])\n",
    "\n",
    "    linear_regression = Sequential([\n",
    "        Dense(1, input_shape=(X_train_preprocessed.shape[1],), activation='linear')\n",
    "    ])\n",
    "\n",
    "    linear_regression.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                               loss='mean_squared_error',\n",
    "                               metrics=['mae'])\n",
    "    ```\n",
    "\n",
    "2. **Model Training:** Both models are trained on the preprocessed training data (`X_train_preprocessed` and `y_train`) with the validation data (`X_valid_preprocessed` and `y_valid`) used for validation during training. The `callbacks` parameter is set to include early stopping for the first model.\n",
    "\n",
    "    ```python\n",
    "    linear_history_early_stopping = linear_regression_early_stopping.fit(X_train_preprocessed, y_train,\n",
    "                                                                          validation_data=(X_valid_preprocessed, y_valid),\n",
    "                                                                          epochs=400,\n",
    "                                                                          batch_size=32,\n",
    "                                                                          verbose=2,\n",
    "                                                                          callbacks=[early_stopping])\n",
    "\n",
    "    linear_history = linear_regression.fit(X_train_preprocessed, y_train,\n",
    "                                           validation_data=(X_valid_preprocessed, y_valid),\n",
    "                                           epochs=400,\n",
    "                                           batch_size=32,\n",
    "                                           verbose=2)\n",
    "    ```\n",
    "\n",
    "3. **Evaluation:** After training, both models are evaluated on the validation set to assess their performance using mean squared error (MSE) and mean absolute error (MAE) metrics.\n",
    "\n",
    "    ```python\n",
    "    linear_evaluation_result_early_stopping = linear_regression_early_stopping.evaluate(X_valid_preprocessed, y_valid, verbose=0)\n",
    "\n",
    "    linear_evaluation_result = linear_regression.evaluate(X_valid_preprocessed, y_valid, verbose=0)\n",
    "    ```\n",
    "\n",
    "These steps facilitate the training and evaluation of linear regression models, allowing us to compare their performance with and without early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_early_stopping = Sequential([\n",
    "    Dense(1, input_shape=(X_train_preprocessed.shape[1],), activation='linear')\n",
    "])\n",
    "\n",
    "linear_regression_early_stopping.compile(optimizer=Adam(learning_rate=0.001),  \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mae']) \n",
    "\n",
    "linear_history_early_stopping = linear_regression_early_stopping.fit(X_train_preprocessed, y_train,\n",
    "                    validation_data=(X_valid_preprocessed, y_valid),\n",
    "                    epochs=400,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "linear_evaluation_result_early_stopping = linear_regression_early_stopping.evaluate(X_valid_preprocessed, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = Sequential([\n",
    "    Dense(1, input_shape=(X_train_preprocessed.shape[1],), activation='linear')\n",
    "])\n",
    "\n",
    "linear_regression.compile(optimizer=Adam(learning_rate=0.001),  \n",
    "              loss='mean_squared_error',  \n",
    "              metrics=['mae']) \n",
    "\n",
    "linear_history = linear_regression.fit(X_train_preprocessed, y_train,\n",
    "                    validation_data=(X_valid_preprocessed, y_valid),\n",
    "                    epochs=400,\n",
    "                    batch_size=32,\n",
    "                    verbose=2)\n",
    "\n",
    "linear_evaluation_result = linear_regression.evaluate(X_valid_preprocessed, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot for linear_history_early_stopping\n",
    "axs[0].plot(linear_history_early_stopping.history['loss'], label='Training Loss')\n",
    "axs[0].plot(linear_history_early_stopping.history['val_loss'], label='Validation Loss')\n",
    "axs[0].set_title('Early Stopping')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot for linear_history\n",
    "axs[1].plot(linear_history.history['loss'], label='Training Loss')\n",
    "axs[1].plot(linear_history.history['val_loss'], label='Validation Loss')\n",
    "axs[1].set_title('Without Early Stopping')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.subplots_adjust(top=0.2, bottom=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) Modeling with Early Stopping\n",
    "\n",
    "This section outlines the process of building, training, and evaluating Multi-Layer Perceptron (MLP) models with early stopping applied to prevent overfitting. The following steps are involved:\n",
    "\n",
    "1. **Model Architecture Definition:** Two MLP models are defined using the Keras Sequential API. Each model consists of multiple dense layers with rectified linear unit (ReLU) activation functions, batch normalization for improving convergence, and dropout layers for regularization.\n",
    "\n",
    "    ```python\n",
    "    mlp_model_early_stopping = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=X_train_preprocessed.shape[1]),\n",
    "        BatchNormalization(),\n",
    "    \n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),  \n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3), \n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    ```\n",
    "\n",
    "2. **Model Compilation:** Both MLP models are compiled using the Adam optimizer, mean squared error (MSE) as the loss function, and mean absolute error (MAE) as the metric to monitor during training.\n",
    "\n",
    "    ```python\n",
    "    mlp_model_early_stopping.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                                      loss='mean_squared_error',\n",
    "                                      metrics=['mae'])\n",
    "    ```\n",
    "\n",
    "3. **Model Training:** The MLP models are trained on the preprocessed training data (`X_train_preprocessed` and `y_train`) with early stopping applied to one of the models. \n",
    "\n",
    "    ```python\n",
    "    mlp_history_early_stopping = mlp_model_early_stopping.fit(X_train_preprocessed, y_train,\n",
    "                                                              validation_data=(X_valid_preprocessed, y_valid),\n",
    "                                                              epochs=400,\n",
    "                                                              batch_size=32,\n",
    "                                                              verbose=2,\n",
    "                                                              callbacks=[early_stopping])\n",
    "    ```\n",
    "\n",
    "4. **Evaluation:** After training, the performance of both models is evaluated on the validation set to assess their predictive capabilities using MSE and MAE metrics.\n",
    "\n",
    "    ```python\n",
    "    mlp_evaluation_result_early_stopping = mlp_model_early_stopping.evaluate(X_valid_preprocessed, y_valid, verbose=0)\n",
    "    ```\n",
    "\n",
    "These steps demonstrate the construction and training of MLP models with early stopping, allowing for better generalization and improved performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_early_stopping = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train_preprocessed.shape[1]),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),  \n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),  \n",
    "\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "mlp_model_early_stopping.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "mlp_history_early_stopping = mlp_model_early_stopping.fit(X_train_preprocessed, y_train,\n",
    "                    validation_data=(X_valid_preprocessed, y_valid),\n",
    "                    epochs=400,  \n",
    "                    batch_size=32,  \n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "mlp_evaluation_result_early_stopping = mlp_model_early_stopping.evaluate(X_valid_preprocessed, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train_preprocessed.shape[1]),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),  \n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3), \n",
    "\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "mlp_history = mlp_model.fit(X_train_preprocessed, y_train,\n",
    "                    validation_data=(X_valid_preprocessed, y_valid),\n",
    "                    epochs=400,  \n",
    "                    batch_size=32, \n",
    "                    verbose=2)\n",
    "\n",
    "mlp_evaluation_result = mlp_model.evaluate(X_valid_preprocessed, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot for mlp_history_early_stopping\n",
    "axs[0].plot(mlp_history_early_stopping.history['loss'], label='Training Loss')\n",
    "axs[0].plot(mlp_history_early_stopping.history['val_loss'], label='Validation Loss')\n",
    "axs[0].set_title('Early Stopping')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot for mlp_history\n",
    "axs[1].plot(mlp_history.history['loss'], label='Training Loss')\n",
    "axs[1].plot(mlp_history.history['val_loss'], label='Validation Loss')\n",
    "axs[1].set_title('Without Early Stopping')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.subplots_adjust(top=0.2, bottom=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) Modeling with Sample Weights\n",
    "\n",
    "This section describes the implementation of sample weights in Multi-Layer Perceptron (MLP) models to address class imbalance issues. The steps involved in incorporating sample weights into the MLP modeling process are outlined below:\n",
    "\n",
    "1. **Computing Sample Weights:** Sample weights are computed using the `compute_sample_weight` function from scikit-learn, with the 'balanced' strategy applied to ensure balanced class weights.\n",
    "\n",
    "    ```python\n",
    "    sample_weights_train = compute_sample_weight('balanced', y_train)\n",
    "    ```\n",
    "\n",
    "2. **Model Architecture Definition:** Two MLP models are defined using the Keras Sequential API. Each model comprises multiple dense layers with ReLU activation functions, batch normalization, and dropout layers for regularization.\n",
    "\n",
    "    ```python\n",
    "    mlp_weight_model_early_stopping = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=X_train_preprocessed.shape[1]),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3), \n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),  \n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    ```\n",
    "\n",
    "3. **Model Compilation:** Both MLP models are compiled using the Adam optimizer, mean squared error (MSE) as the loss function, mean absolute error (MAE) as the metric, and 'temporal' sample weight mode to incorporate sample weights during training.\n",
    "\n",
    "    ```python\n",
    "    mlp_weight_model_early_stopping.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                                             loss='mean_squared_error',\n",
    "                                             metrics=['mae'],\n",
    "                                             sample_weight_mode='temporal')\n",
    "    ```\n",
    "\n",
    "4. **Model Training:** The MLP models are trained on the preprocessed training data (`X_train_preprocessed` and `y_train`) with early stopping applied to one of the models. Sample weights are passed as an argument to the `fit` method to incorporate them during training.\n",
    "\n",
    "    ```python\n",
    "    weight_model_history_early_stopping = mlp_weight_model_early_stopping.fit(X_train_preprocessed, y_train,\n",
    "                                                                                validation_data=(X_valid_preprocessed, y_valid),\n",
    "                                                                                epochs=400,\n",
    "                                                                                batch_size=32,\n",
    "                                                                                verbose=2,\n",
    "                                                                                sample_weight=sample_weights_train,\n",
    "                                                                                callbacks=[early_stopping])\n",
    "    ```\n",
    "\n",
    "5. **Evaluation:** After training, the performance of both models is evaluated on the validation set to assess their predictive capabilities using MSE and MAE metrics.\n",
    "\n",
    "    ```python\n",
    "    weight_model_evaluation_result_early_stopping = mlp_weight_model_early_stopping.evaluate(X_valid_preprocessed, y_valid, verbose=0)\n",
    "    ```\n",
    "\n",
    "These steps demonstrate the integration of sample weights into MLP modeling to address class imbalance issues and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights_train = compute_sample_weight('balanced', y_train)\n",
    "mlp_weight_model_early_stopping = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train_preprocessed.shape[1]),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3), \n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),  \n",
    "\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "mlp_weight_model_early_stopping.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mae'],\n",
    "              sample_weight_mode='temporal') \n",
    "\n",
    "weight_model_history_early_stopping = mlp_weight_model_early_stopping.fit(X_train_preprocessed, y_train,\n",
    "                    validation_data=(X_valid_preprocessed, y_valid),\n",
    "                    epochs=400,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    sample_weight=sample_weights_train,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "weight_model_evaluation_result_early_stopping = mlp_weight_model_early_stopping.evaluate(X_valid_preprocessed, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_weight_model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train_preprocessed.shape[1]),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),  \n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),  \n",
    "\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "mlp_weight_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                         sample_weight_mode='temporal',\n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mae'])\n",
    "\n",
    "weight_model_history = mlp_weight_model.fit(X_train_preprocessed, y_train,\n",
    "                    validation_data=(X_valid_preprocessed, y_valid),\n",
    "                    epochs=400,\n",
    "                    batch_size=32,\n",
    "                    sample_weight=sample_weights_train,\n",
    "                    verbose=2)\n",
    "\n",
    "weight_model_evaluation_result = mlp_weight_model.evaluate(X_valid_preprocessed, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot for weight_model_history_early_stopping\n",
    "axs[0].plot(weight_model_history_early_stopping.history['loss'], label='Training Loss')\n",
    "axs[0].plot(weight_model_history_early_stopping.history['val_loss'], label='Validation Loss')\n",
    "axs[0].set_title('Early Stopping')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot for weight_model_history\n",
    "axs[1].plot(weight_model_history.history['loss'], label='Training Loss')\n",
    "axs[1].plot(weight_model_history.history['val_loss'], label='Validation Loss')\n",
    "axs[1].set_title('Without Early Stopping')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.subplots_adjust(top=0.4, bottom=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot for models with early stopping\n",
    "axs[0].plot(linear_history_early_stopping.history['loss'], label='Linear')\n",
    "axs[0].plot(mlp_history_early_stopping.history['loss'], label='MLP')\n",
    "axs[0].plot(weight_model_history_early_stopping.history['loss'], label='MLP with Weight')\n",
    "axs[0].set_title('Models with Early Stopping')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot for models without early stopping\n",
    "axs[1].plot(linear_history.history['loss'], label='Linear')\n",
    "axs[1].plot(mlp_history.history['loss'], label='MLP')\n",
    "axs[1].plot(weight_model_history.history['loss'], label='MLP with Weight')\n",
    "axs[1].set_title('Models without Early Stopping')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot for all models\n",
    "axs[2].plot(linear_history_early_stopping.history['loss'], label='Linear (Early Stopping)')\n",
    "axs[2].plot(mlp_history_early_stopping.history['loss'], label='MLP (Early Stopping)')\n",
    "axs[2].plot(weight_model_history_early_stopping.history['loss'], label='MLP with Weight (Early Stopping)')\n",
    "axs[2].plot(linear_history.history['loss'], label='Linear')\n",
    "axs[2].plot(mlp_history.history['loss'], label='MLP')\n",
    "axs[2].plot(weight_model_history.history['loss'], label='MLP with Weight')\n",
    "axs[2].set_title('All Models')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Loss')\n",
    "axs[2].grid(True)\n",
    "axs[2].legend()\n",
    "\n",
    "plt.subplots_adjust(top=0.4, bottom=0.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN) Modeling with and without Early Stopping\n",
    "\n",
    "This section outlines the implementation of Convolutional Neural Network (CNN) models with early stopping for regression tasks. The process involves the following steps:\n",
    "\n",
    "1. **CNN Model Definition:** Two CNN models are defined using the Keras Sequential API. Each model consists of convolutional layers with ReLU activation functions, max-pooling layers for downsampling, and dense layers for regression.\n",
    "\n",
    "    ```python\n",
    "    cnn_model_early_stopping = Sequential([\n",
    "        Conv1D(32, 3, activation='relu', input_shape=(248, 1)),\n",
    "        MaxPooling1D(2),\n",
    "\n",
    "        Conv1D(64, 3, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "\n",
    "        Conv1D(128, 3, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "\n",
    "        Dense(1)\n",
    "    ])\n",
    "    ```\n",
    "\n",
    "2. **Model Compilation:** Both CNN models are compiled using the Adam optimizer, mean squared error (MSE) as the loss function, and mean absolute error (MAE) as the metric.\n",
    "\n",
    "    ```python\n",
    "    cnn_model_early_stopping.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss='mean_squared_error',\n",
    "                      metrics=['mae'])\n",
    "    ```\n",
    "\n",
    "3. **Model Training:** The CNN models are trained on the preprocessed training data with early stopping applied to one of the models. The `fit` method is used to train the models with specified epochs, batch size, and verbose mode.\n",
    "\n",
    "    ```python\n",
    "    cnn_model_history_early_stopping = cnn_model_early_stopping.fit(X_train_preprocessed, y_train,\n",
    "                        validation_data=(X_valid_preprocessed, y_valid),\n",
    "                        epochs=400,\n",
    "                        batch_size=32,\n",
    "                        verbose=2,\n",
    "                        callbacks=[early_stopping])\n",
    "    ```\n",
    "\n",
    "4. **Evaluation:** After training, the performance of both models is evaluated on the validation set using MSE and MAE metrics.\n",
    "\n",
    "    ```python\n",
    "    cnn_model_evalutaion_result_early_stopping = cnn_model_early_stopping.evaluate(X_valid_preprocessed, y_valid, verbose=0)\n",
    "    ```\n",
    "\n",
    "These steps illustrate the process of implementing CNN models with early stopping for regression tasks, providing insights into model architecture, compilation, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, valid_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "x_train, y_train = data_X.loc[train_indices, :], y.loc[train_indices]\n",
    "x_valid, y_valid = data_X.loc[valid_indices, :], y.loc[valid_indices]\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(x_train)\n",
    "X_valid_preprocessed = preprocessor.transform(x_valid)\n",
    "\n",
    "cnn_model_early_stopping = Sequential([\n",
    "    Conv1D(32, 3, activation='relu', input_shape=(248, 1)),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Conv1D(64, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "cnn_model_early_stopping.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "cnn_model_history_early_stopping = cnn_model_early_stopping.fit(X_train_preprocessed, y_train,\n",
    "                    validation_data=(X_valid_preprocessed, y_valid),\n",
    "                    epochs=400,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "cnn_model_evalutaion_result_early_stopping = cnn_model_early_stopping.evaluate(X_valid_preprocessed, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential([\n",
    "    Conv1D(32, 3, activation='relu', input_shape=(248, 1)),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Conv1D(64, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_model_history = cnn_model.fit(X_train_preprocessed, y_train,\n",
    "                    validation_data=(X_valid_preprocessed, y_valid),\n",
    "                    epochs=400,\n",
    "                    batch_size=32,\n",
    "                    verbose=2)\n",
    "\n",
    "cnn_model_evalutaion_result = cnn_model.evaluate(X_valid_preprocessed, y_valid, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axs[0].plot(cnn_model_history_early_stopping.history['loss'], label='Training Loss')\n",
    "axs[0].plot(cnn_model_history_early_stopping.history['val_loss'], label='Validation Loss')\n",
    "axs[0].set_title('Early Stopping')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(cnn_model_history.history['loss'], label='Training Loss')\n",
    "axs[1].plot(cnn_model_history.history['val_loss'], label='Validation Loss')\n",
    "axs[1].set_title('Without Early Stopping')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.subplots_adjust(top=0.1, bottom=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing All Models\n",
    "## Training Loss Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear Regression model loss is : {linear_history_early_stopping.history['loss'][-1]}\\nMLP model loss is: {mlp_history_early_stopping.history['loss'][-1]}\\nMLP model with updated weights loss is : {weight_model_history_early_stopping.history['loss'][-1]}\\nCNN model loss is: {cnn_model_history_early_stopping.history['loss'][-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_objects_early_stopping = [linear_history_early_stopping, mlp_history_early_stopping, \n",
    "                   weight_model_history_early_stopping, cnn_model_history_early_stopping]\n",
    "\n",
    "model_names = ['Linear Regression', 'MLP', 'MLP with Updated Weights', 'CNN']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for history, name in zip(history_objects_early_stopping, model_names):\n",
    "    plt.plot(history.history['loss'], label=f'{name} Training Loss', linestyle='--')\n",
    "\n",
    "plt.title('Training Loss Early Stopping')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loss Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear Regression model validation loss is : {linear_history_early_stopping.history['val_loss'][-1]}\\nMLP model validation loss is: {mlp_history_early_stopping.history['val_loss'][-1]}\\nMLP model with updated weights validation loss is : {weight_model_history_early_stopping.history['val_loss'][-1]}\\nCNN model validation loss is: {cnn_model_history_early_stopping.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for history, name in zip(history_objects_early_stopping, model_names):\n",
    "    plt.plot(history.history['val_loss'], label=f'{name} Validation Loss', linestyle='--')\n",
    "\n",
    "plt.title('Validation Loss Early Stopping')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear Regression model loss is : {linear_history.history['loss'][-1]}\\nMLP model loss is: {mlp_history.history['loss'][-1]}\\nMLP model with updated weights loss is : {weight_model_history.history['loss'][-1]}\\nCNN model loss is: {cnn_model_history.history['loss'][-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_objects = [linear_history, mlp_history, \n",
    "                   weight_model_history, cnn_model_history]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for history, name in zip(history_objects, model_names):\n",
    "    plt.plot(history.history['loss'], label=f'{name} Training Loss', linestyle='--')\n",
    "\n",
    "plt.title('Training Loss Without Early Stopping')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loss Without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear Regression model validation loss is : {linear_history.history['val_loss'][-1]}\\nMLP model validation loss is: {mlp_history.history['val_loss'][-1]}\\nMLP model with updated weights validation loss is : {weight_model_history.history['val_loss'][-1]}\\nCNN model validation loss is: {cnn_model_history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for history, name in zip(history_objects, model_names):\n",
    "    plt.plot(history.history['val_loss'], label=f'{name} Validation Loss', linestyle='--')\n",
    "\n",
    "plt.title('Validation Loss Early Stopping')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
